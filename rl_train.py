#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬ï¼šä½¿ç”¨ PPO è®­ç»ƒæœºæ¢°è‡‚æ¥è§¦ç»¿è‰²å¶ç‰‡

ç”¨æ³•ï¼š
  conda activate bullet312
  pip install stable-baselines3 gymnasium
  python rl_train.py --train  # è®­ç»ƒæ¨¡å¼
  python rl_train.py --test   # æµ‹è¯•å·²è®­ç»ƒæ¨¡å‹
"""

import os
import time
import math
import tempfile
import argparse
import numpy as np
import gymnasium as gym
from gymnasium import spaces
import pybullet as p
import pybullet_data
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# å¤ç”¨ URDF æ¨¡æ¿
URDF_TEMPLATE = """
<?xml version="1.0"?>
<robot name="three_dof_arm">
  <link name="base_link">
    <inertial><origin xyz="0 0 0"/><mass value="1"/><inertia ixx="0.001" iyy="0.001" izz="0.001" ixy="0" ixz="0" iyz="0"/></inertial>
    <visual><origin xyz="0 0 0"/><geometry><box size="0.12 0.12 0.06"/></geometry><material name="grey"><color rgba="0.7 0.7 0.7 1"/></material></visual>
    <collision><origin xyz="0 0 0"/><geometry><box size="0.12 0.12 0.06"/></geometry></collision>
  </link>

  <joint name="joint1" type="revolute">
    <parent link="base_link"/><child link="link1"/>
    <origin xyz="0 0 0.03"/><axis xyz="0 0 1"/>
    <limit lower="-3.14159" upper="3.14159" effort="20" velocity="6"/>
    <dynamics damping="0.02" friction="0.0"/>
  </joint>
  <link name="link1">
    <inertial><origin xyz="0 0 0.11"/><mass value="0.4"/><inertia ixx="0.002" iyy="0.002" izz="0.002" ixy="0" ixz="0" iyz="0"/></inertial>
    <visual><origin xyz="0 0 0.11"/><geometry><box size="0.04 0.04 0.22"/></geometry><material name="blue"><color rgba="0.2 0.4 0.9 1"/></material></visual>
    <collision><origin xyz="0 0 0.11"/><geometry><box size="0.04 0.04 0.22"/></geometry></collision>
  </link>

  <joint name="joint2" type="revolute">
    <parent link="link1"/><child link="link2"/>
    <origin xyz="0 0 0.22"/><axis xyz="0 1 0"/>
    <limit lower="-1.745" upper="1.745" effort="20" velocity="6"/>
    <dynamics damping="0.02" friction="0.0"/>
  </joint>
  <link name="link2">
    <inertial><origin xyz="0.09 0 0"/><mass value="0.35"/><inertia ixx="0.002" iyy="0.002" izz="0.002" ixy="0" ixz="0" iyz="0"/></inertial>
    <visual><origin xyz="0.09 0 0"/><geometry><box size="0.18 0.035 0.035"/></geometry><material name="orange"><color rgba="0.95 0.55 0.2 1"/></material></visual>
    <collision><origin xyz="0.09 0 0"/><geometry><box size="0.18 0.035 0.035"/></geometry></collision>
  </link>

  <joint name="joint3" type="revolute">
    <parent link="link2"/><child link="link3"/>
    <origin xyz="0.18 0 0"/><axis xyz="0 1 0"/>
    <limit lower="-2.094" upper="2.094" effort="20" velocity="6"/>
    <dynamics damping="0.02" friction="0.0"/>
  </joint>
  <link name="link3">
    <inertial><origin xyz="0.08 0 0"/><mass value="0.3"/><inertia ixx="0.002" iyy="0.002" izz="0.002" ixy="0" ixz="0" iyz="0"/></inertial>
    <visual><origin xyz="0.08 0 0"/><geometry><box size="0.16 0.03 0.03"/></geometry><material name="green"><color rgba="0.2 0.8 0.4 1"/></material></visual>
    <collision><origin xyz="0.08 0 0"/><geometry><box size="0.16 0.03 0.03"/></geometry></collision>
  </link>

  <joint name="ee_fixed" type="fixed"><parent link="link3"/><child link="ee"/><origin xyz="0.16 0 0"/></joint>
  <link name="ee">
    <visual><origin xyz="0 0 0"/><geometry><box size="0.03 0.03 0.03"/></geometry><material name="red"><color rgba="0.9 0.2 0.2 1"/></material></visual>
    <collision><origin xyz="0 0 0"/><geometry><box size="0.03 0.03 0.03"/></geometry></collision>
  </link>
</robot>
"""


def write_urdf_tmp(text: str) -> str:
    """å†™å…¥ä¸´æ—¶ URDF æ–‡ä»¶"""
    tmpdir = tempfile.mkdtemp(prefix="arm3dof_rl_")
    urdf_path = os.path.join(tmpdir, "arm3dof.urdf")
    with open(urdf_path, "w", encoding="utf-8") as f:
        f.write(text)
    return urdf_path


class ArmReachEnv(gym.Env):
    """
    æœºæ¢°è‡‚æ¥è§¦å¶ç‰‡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
    
    è§‚å¯Ÿç©ºé—´ï¼š
      - 3 ä¸ªå…³èŠ‚è§’åº¦ (rad)
      - 3 ä¸ªå…³èŠ‚è§’é€Ÿåº¦ (rad/s)
      - æœ«ç«¯ä½ç½® (x, y, z)
      - ç›®æ ‡ä½ç½® (x, y, z)
      - æœ«ç«¯åˆ°ç›®æ ‡çš„è·ç¦»
    
    åŠ¨ä½œç©ºé—´ï¼š
      - 3 ä¸ªå…³èŠ‚çš„ç›®æ ‡è§’åº¦å¢é‡ (è¿ç»­åŠ¨ä½œ)
    
    å¥–åŠ±ï¼š
      - è·ç¦»å¥–åŠ±ï¼šè¶Šæ¥è¿‘ç›®æ ‡å¥–åŠ±è¶Šé«˜
      - æ¥è§¦å¥–åŠ±ï¼šæœ«ç«¯ä¸å¶ç‰‡æ¥è§¦æ—¶ç»™äºˆå¤§é¢å¥–åŠ±
      - æƒ©ç½šï¼šè¿‡å¤§çš„åŠ¨ä½œã€è¶…å‡ºå…³èŠ‚é™åˆ¶
    """
    
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 60}
    
    def __init__(self, render_mode=None, max_steps=500):
        super().__init__()
        
        self.render_mode = render_mode
        self.max_steps = max_steps
        self.current_step = 0
        
        # PyBullet è¿æ¥
        if render_mode == "human":
            self.physics_client = p.connect(p.GUI)
            p.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)
        else:
            self.physics_client = p.connect(p.DIRECT)
        
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.81)
        p.setTimeStep(1/240)
        
        # åŠ è½½åœºæ™¯
        self.plane_id = p.loadURDF("plane.urdf")
        urdf_path = write_urdf_tmp(URDF_TEMPLATE)
        self.arm_id = p.loadURDF(urdf_path, basePosition=[0, 0, 0], useFixedBase=True,
                                 flags=p.URDF_USE_SELF_COLLISION)
        
        # å¶ç‰‡ï¼ˆç›®æ ‡ï¼‰
        leaf_thickness = 0.004
        leaf_half_extents = [0.12, 0.08, leaf_thickness / 2]
        leaf_vis = p.createVisualShape(p.GEOM_BOX, halfExtents=leaf_half_extents, 
                                       rgbaColor=[0.2, 0.7, 0.2, 1])
        leaf_col = p.createCollisionShape(p.GEOM_BOX, halfExtents=leaf_half_extents)
        self.leaf_pos = [0.35, 0.0, 0.35]
        self.leaf_id = p.createMultiBody(baseMass=0, baseCollisionShapeIndex=leaf_col, 
                                        baseVisualShapeIndex=leaf_vis,
                                        basePosition=self.leaf_pos, 
                                        baseOrientation=p.getQuaternionFromEuler([0, 0.0, 0]))
        
        # å…³èŠ‚ä¿¡æ¯
        self.control_joints = [0, 1, 2]
        
        # æ‰¾åˆ°æœ«ç«¯æ‰§è¡Œå™¨é“¾æ¥ç´¢å¼•
        num_joints = p.getNumJoints(self.arm_id)
        self.ee_link_index = num_joints - 1  # æœ€åä¸€ä¸ªé“¾æ¥æ˜¯æœ«ç«¯æ‰§è¡Œå™¨
        print(f"å…³èŠ‚æ•°: {num_joints}, æœ«ç«¯é“¾æ¥ç´¢å¼•: {self.ee_link_index}")
        
        # å…³èŠ‚é™åˆ¶ (rad)
        self.joint_lower_limits = np.array([-3.14159, -1.745, -2.094])
        self.joint_upper_limits = np.array([3.14159, 1.745, 2.094])
        
        # å®šä¹‰è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´
        # è§‚å¯Ÿï¼š3 å…³èŠ‚è§’ + 3 å…³èŠ‚é€Ÿåº¦ + 3 æœ«ç«¯ä½ç½® + 3 ç›®æ ‡ä½ç½® + 1 è·ç¦» = 13
        obs_low = np.array(
            list(self.joint_lower_limits) + [-10]*3 +  # å…³èŠ‚è§’ + é€Ÿåº¦
            [-1, -1, 0] + [-1, -1, 0] + [0],  # æœ«ç«¯ä½ç½® + ç›®æ ‡ä½ç½® + è·ç¦»
            dtype=np.float32
        )
        obs_high = np.array(
            list(self.joint_upper_limits) + [10]*3 +
            [1, 1, 1] + [1, 1, 1] + [2],
            dtype=np.float32
        )
        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)
        
        # åŠ¨ä½œï¼š3 ä¸ªå…³èŠ‚çš„è§’åº¦å¢é‡ï¼ˆ-0.1 åˆ° 0.1 radï¼‰
        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(3,), dtype=np.float32)
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.target_angles = np.zeros(3)
        self.prev_distance = None
        
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        self.current_step = 0
        
        # éšæœºåˆå§‹å…³èŠ‚è§’åº¦
        if self.np_random is not None:
            init_angles = self.np_random.uniform(
                self.joint_lower_limits * 0.5,
                self.joint_upper_limits * 0.5
            )
        else:
            init_angles = np.zeros(3)
        
        # é‡ç½®å…³èŠ‚çŠ¶æ€
        for i, joint_idx in enumerate(self.control_joints):
            p.resetJointState(self.arm_id, joint_idx, init_angles[i])
        
        self.target_angles = init_angles.copy()
        
        # æ‰§è¡Œå‡ æ­¥è®©ç‰©ç†ç¨³å®š
        for _ in range(10):
            p.stepSimulation()
        
        obs = self._get_obs()
        self.prev_distance = self._get_distance_to_target()
        
        return obs, {}
    
    def step(self, action):
        self.current_step += 1
        
        # åº”ç”¨åŠ¨ä½œï¼ˆå¢é‡ï¼‰
        self.target_angles += action
        self.target_angles = np.clip(self.target_angles, 
                                     self.joint_lower_limits, 
                                     self.joint_upper_limits)
        
        # å‘é€å…³èŠ‚æ§åˆ¶å‘½ä»¤
        p.setJointMotorControlArray(
            self.arm_id, 
            self.control_joints, 
            p.POSITION_CONTROL,
            targetPositions=self.target_angles,
            positionGains=[0.8]*3,
            velocityGains=[0.3]*3,
            forces=[50]*3
        )
        
        # æ‰§è¡Œä»¿çœŸæ­¥
        p.stepSimulation()
        if self.render_mode == "human":
            time.sleep(1/60)  # é™ä½åˆ°60Hzï¼Œè®©åŠ¨ä½œæ›´æ¸…æ™°å¯è§
        
        # è·å–æ–°çŠ¶æ€
        obs = self._get_obs()
        
        # è®¡ç®—å¥–åŠ±
        reward, info = self._compute_reward()
        
        # ç»ˆæ­¢æ¡ä»¶
        terminated = info.get("success", False)
        truncated = self.current_step >= self.max_steps
        
        return obs, reward, terminated, truncated, info
    
    def _get_obs(self):
        """è·å–è§‚å¯Ÿ"""
        # å…³èŠ‚çŠ¶æ€
        joint_states = [p.getJointState(self.arm_id, i) for i in self.control_joints]
        joint_angles = np.array([js[0] for js in joint_states], dtype=np.float32)
        joint_vels = np.array([js[1] for js in joint_states], dtype=np.float32)
        
        # æœ«ç«¯ä½ç½® - ä½¿ç”¨ computeForwardKinematics ç¡®ä¿æ•°æ®æ›´æ–°
        p.performCollisionDetection()
        ee_state = p.getLinkState(self.arm_id, self.ee_link_index, computeLinkVelocity=0)
        
        if ee_state is None or ee_state[0] is None:
            # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä½ç½®
            ee_pos = np.array([0.0, 0.0, 0.5], dtype=np.float32)
        else:
            ee_pos = np.array(ee_state[0], dtype=np.float32)
        
        # ç›®æ ‡ä½ç½®
        target_pos = np.array(self.leaf_pos, dtype=np.float32)
        
        # è·ç¦»
        distance = np.linalg.norm(ee_pos - target_pos)
        
        obs = np.concatenate([
            joint_angles,
            joint_vels,
            ee_pos,
            target_pos,
            [distance]
        ])
        
        return obs
    
    def _get_distance_to_target(self):
        """è®¡ç®—æœ«ç«¯åˆ°ç›®æ ‡çš„è·ç¦»"""
        p.performCollisionDetection()
        ee_state = p.getLinkState(self.arm_id, self.ee_link_index, computeLinkVelocity=0)
        
        if ee_state is None or ee_state[0] is None:
            return 999.0  # è¿”å›ä¸€ä¸ªå¤§å€¼
        
        ee_pos = np.array(ee_state[0])
        target_pos = np.array(self.leaf_pos)
        return np.linalg.norm(ee_pos - target_pos)
    
    def _compute_reward(self):
        """è®¡ç®—å¥–åŠ±"""
        info = {}
        
        # å½“å‰è·ç¦»
        distance = self._get_distance_to_target()
        
        # åŸºç¡€è·ç¦»å¥–åŠ±ï¼šè·ç¦»è¶Šå°å¥–åŠ±è¶Šé«˜
        distance_reward = -distance * 10.0
        
        # è¿›æ­¥å¥–åŠ±ï¼šå¦‚æœæ¯”ä¸Šä¸€æ­¥æ›´æ¥è¿‘
        progress_reward = 0.0
        if self.prev_distance is not None:
            progress = self.prev_distance - distance
            progress_reward = progress * 50.0
        self.prev_distance = distance
        
        # æ¥è§¦æ£€æµ‹
        contact_points = p.getContactPoints(self.arm_id, self.leaf_id)
        contact_reward = 0.0
        success = False
        
        if len(contact_points) > 0:
            # æ£€æŸ¥æ˜¯å¦æœ«ç«¯æ¥è§¦
            for cp in contact_points:
                if cp[3] == self.ee_link_index:  # æœ«ç«¯é“¾æ¥æ¥è§¦
                    contact_reward = 100.0
                    success = True
                    info["success"] = True
                    break
        
        # åˆ°è¾¾å¥–åŠ±ï¼šè·ç¦»å°äºé˜ˆå€¼
        reach_reward = 0.0
        if distance < 0.05:
            reach_reward = 50.0
            if distance < 0.02:
                reach_reward = 100.0
        
        # åŠ¨ä½œå¹³æ»‘æƒ©ç½šï¼ˆå¯é€‰ï¼‰
        action_penalty = 0.0
        
        # æ€»å¥–åŠ±
        reward = distance_reward + progress_reward + contact_reward + reach_reward + action_penalty
        
        info["distance"] = distance
        info["contact"] = len(contact_points) > 0
        
        return reward, info
    
    def close(self):
        p.disconnect(self.physics_client)


def train(args):
    """è®­ç»ƒæ™ºèƒ½ä½“"""
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒ
    def make_env():
        return ArmReachEnv(render_mode=None, max_steps=500)
    
    env = DummyVecEnv([make_env])
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)
    
    # åˆ›å»ºæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        verbose=1,
        tensorboard_log="./ppo_arm_tensorboard/"
    )
    
    # å›è°ƒï¼šå®šæœŸä¿å­˜
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path="./checkpoints/",
        name_prefix="ppo_arm"
    )
    
    # è®­ç»ƒ
    total_timesteps = args.timesteps
    print(f"\nå¼€å§‹è®­ç»ƒ {total_timesteps} æ­¥...")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True
        )
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£… tqdm/richï¼Œç¦ç”¨è¿›åº¦æ¡
        print("æ³¨æ„ï¼šè¿›åº¦æ¡ä¸å¯ç”¨ï¼Œå¦‚éœ€å¯ç”¨è¯·å®‰è£…: pip install tqdm rich")
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=False
        )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save("ppo_arm_final")
    env.save("vec_normalize.pkl")
    print("\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ppo_arm_final.zip")


def test(args):
    """æµ‹è¯•å·²è®­ç»ƒçš„æ™ºèƒ½ä½“"""
    print("=" * 60)
    print("æµ‹è¯•å·²è®­ç»ƒçš„æ™ºèƒ½ä½“")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model_path = args.model_path or "ppo_arm_final"
    print(f"åŠ è½½æ¨¡å‹ï¼š{model_path}")
    
    try:
        model = PPO.load(model_path)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    except FileNotFoundError:
        print(f"âœ— æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{model_path}.zip")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒï¼špython rl_train.py --train")
        return
    
    # åˆ›å»ºå¯è§†åŒ–ç¯å¢ƒ
    env = ArmReachEnv(render_mode="human", max_steps=1000)
    
    # å¦‚æœæœ‰å½’ä¸€åŒ–å‚æ•°ï¼ŒåŠ è½½å®ƒ
    try:
        env = VecNormalize.load("vec_normalize.pkl", DummyVecEnv([lambda: env]))
        env.training = False
        env.norm_reward = False
        print("âœ“ å½’ä¸€åŒ–å‚æ•°åŠ è½½æˆåŠŸ")
    except FileNotFoundError:
        print("âš  æœªæ‰¾åˆ°å½’ä¸€åŒ–å‚æ•°ï¼Œä½¿ç”¨æœªå½’ä¸€åŒ–çš„ç¯å¢ƒ")
        pass
    
    # è¿è¡Œæµ‹è¯•å›åˆ
    num_episodes = args.episodes
    for episode in range(num_episodes):
        obs = env.reset()
        done = False
        total_reward = 0
        steps = 0
        text_id = None
        
        print(f"\n{'='*60}")
        print(f"ç¬¬ {episode + 1}/{num_episodes} å›åˆ")
        print(f"{'='*60}")
        print("ğŸ‘€ è§‚å¯Ÿæœºæ¢°è‡‚å¦‚ä½•è‡ªä¸»å­¦ä¹ æ¥è§¦ç»¿è‰²å¶ç‰‡...\n")
        
        # è·å–åŸå§‹ç¯å¢ƒç”¨äºæ˜¾ç¤ºä¿¡æ¯
        if isinstance(env, VecNormalize):
            raw_env = env.venv.envs[0]
        else:
            raw_env = env
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            total_reward += reward
            steps += 1
            
            # æ˜¾ç¤ºå®æ—¶ä¿¡æ¯åˆ°GUI
            try:
                dist = raw_env._get_distance_to_target()
                status_text = f"æ­¥æ•°: {steps} | è·ç¦»: {dist:.3f}m"
                
                # ç§»é™¤æ—§çš„æ–‡å­—
                if text_id is not None:
                    try:
                        p.removeUserDebugItem(text_id)
                    except:
                        pass
                
                # æ·»åŠ æ–°çš„æ–‡å­—æç¤º
                text_id = p.addUserDebugText(
                    status_text,
                    [0.3, 0.3, 0.6],
                    textColorRGB=[1, 1, 0],
                    textSize=1.8
                )
                
                # æ¯5æ­¥åœ¨æ§åˆ¶å°è¾“å‡ºä¸€æ¬¡
                if steps % 5 == 0:
                    print(f"  æ­¥éª¤ {steps:3d}: è·ç¦» {dist:.4f}m")
            except:
                pass
            
            # é¢å¤–å»¶è¿Ÿï¼Œè®©æ¼”ç¤ºæ›´æ…¢æ›´æ¸…æ™°
            time.sleep(0.03)  # æ¯æ­¥é¢å¤–æš‚åœ30ms
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if isinstance(info, list):
                info = info[0]
            if info.get("success", False):
                # æ¸…é™¤çŠ¶æ€æ–‡å­—
                if text_id is not None:
                    try:
                        p.removeUserDebugItem(text_id)
                    except:
                        pass
                
                # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                try:
                    success_text_id = p.addUserDebugText(
                        "ğŸ‰ æˆåŠŸæ¥è§¦ï¼",
                        [0.35, 0.0, 0.5],
                        textColorRGB=[0, 1, 0],
                        textSize=2.5
                    )
                except:
                    pass
                
                # å¤„ç†å¯èƒ½çš„ numpy æ•°ç»„
                reward_val = float(total_reward.item()) if hasattr(total_reward, 'item') else float(total_reward)
                print(f"\n{'='*60}")
                print("ğŸ‰ æˆåŠŸï¼")
                print(f"{'='*60}")
                print(f"  æ­¥æ•°: {steps}")
                print(f"  æ€»å¥–åŠ±: {reward_val:.2f}")
                print(f"  æœ«ç«¯æˆåŠŸæ¥è§¦åˆ°ç»¿è‰²å¶ç‰‡ï¼")
                print(f"{'='*60}")
                
                time.sleep(2)  # æˆåŠŸåæš‚åœ2ç§’æ¬£èµ
                
                try:
                    p.removeUserDebugItem(success_text_id)
                except:
                    pass
                break
        
        if not info.get("success", False):
            reward_val = float(total_reward.item()) if hasattr(total_reward, 'item') else float(total_reward)
            dist_val = info.get('distance', 'N/A')
            print(f"\n{'='*60}")
            print("âŒ æœªæˆåŠŸ")
            print(f"{'='*60}")
            print(f"  æ­¥æ•°: {steps}")
            print(f"  æ€»å¥–åŠ±: {reward_val:.2f}")
            if dist_val != 'N/A':
                print(f"  æœ€ç»ˆè·ç¦»: {dist_val:.3f}m")
            print(f"{'='*60}")
        
        # æ¸…é™¤æ‰€æœ‰æ–‡å­—
        if text_id is not None:
            try:
                p.removeUserDebugItem(text_id)
            except:
                pass
        
        # å›åˆé—´éš”
        if episode < num_episodes - 1:
            print("\nâ³ 3ç§’åå¼€å§‹ä¸‹ä¸€å›åˆ...")
            time.sleep(3)
    
    env.close()


def main():
    parser = argparse.ArgumentParser(description="å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºæ¢°è‡‚æ¥è§¦å¶ç‰‡")
    parser.add_argument("--train", action="store_true", help="è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼")
    parser.add_argument("--timesteps", type=int, default=200000, help="è®­ç»ƒæ€»æ­¥æ•°")
    parser.add_argument("--episodes", type=int, default=5, help="æµ‹è¯•å›åˆæ•°")
    parser.add_argument("--model-path", type=str, default=None, help="æµ‹è¯•æ—¶åŠ è½½çš„æ¨¡å‹è·¯å¾„")
    
    args = parser.parse_args()
    
    if args.train:
        train(args)
    elif args.test:
        test(args)
    else:
        print("è¯·æŒ‡å®šæ¨¡å¼ï¼š--train æˆ– --test")
        parser.print_help()


if __name__ == "__main__":
    main()
