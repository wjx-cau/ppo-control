# Qwen 提示词优化说明

## 🎯 优化目标
**防止 Qwen 产生幻觉**：避免她认为"之前看到的"就是"当前看到的"。

## 📊 优化策略
只告诉 Qwen **上次的控制动作**，不告诉她**上次看到了什么**，让她：
1. ✅ 理解控制效果（例如：我右转了，画面确实向左移动了）
2. ✅ 判断控制结果（例如：转太多了，目标丢失了）
3. ❌ 不会记住具体的历史内容导致混淆

---

## 🔄 优化前后对比

### ❌ 优化前（冗长，容易产生幻觉）

**系统提示词**：
```
你是一个装在机械臂末端的摄像头AI控制器。

**重要说明：这是一个连续控制任务！**
你需要根据历史记录（上次控制动作和看到的内容）来分析当前的观察结果，
理解控制与观察之间的关系。

# 机械臂控制说明：
## 可用控制键（每个键的速度为 120°/秒）：
- **I键**：J1底座逆时针旋转（摄像头向左转）
- **K键**：J1底座顺时针旋转（摄像头向右转）
- **J键**：J2肩部抬起（摄像头向上看）
- **L键**：J2肩部下降（摄像头向下看）
... (更多冗长的说明)

#环境说明：
## 你如果看到紫色的墙壁，那就是仿真环境中的天空，蓝色棋盘是仿真环境中的地面，
黑色圆盘是植物的底座，绿色的是植物的叶子，就在黑色圆盘上方...
(一大段环境描述)
```

**历史上下文**：
```
# 上一次的控制与观察：
- **上次控制**: K 键 0.50秒
- **上次看到**: 看到绿色植物，有多片叶子  ← 问题：告诉了历史观察内容

请根据上次的控制动作，分析当前画面的变化，理解控制与观察的关系。
```

**问题**：
1. ❌ 提示词太长，容易让模型分心
2. ❌ 告诉了"上次看到的内容"，导致模型混淆历史和现在
3. ❌ 过于详细的环境说明，反而让模型不知道重点

---

### ✅ 优化后（简洁，专注当前）

**系统提示词**：
```
你是机械臂末端的摄像头AI。

# 任务目标
找到并对准绿色植物，让植物在画面中心。

# 环境说明
- 紫色墙壁 = 天空
- 蓝色棋盘 = 地面
- 黑色圆盘 = 植物底座
- 绿色物体 = 植物（目标）

# 控制按键（速度：120°/秒）
- I键：底座左转 | K键：底座右转
- J键：抬头看 | L键：低头看
- U键：微调向上 | O键：微调向下
- Q键：轨道逆时针 | P键：轨道顺时针
- Y键：升高 | H键：降低

# 控制时长
- 大幅搜索：0.5~1.0秒（60°~120°）
- 小幅调整：0.15~0.3秒（18°~36°）

# 输出格式
{
  "summary": "描述你当前看到的内容",
  "control": {
    "key": "K",
    "duration": 0.3
  }
}

只输出JSON，不要其他文字。
```

**历史上下文**：
```
# 上次控制动作
我刚才执行了：K 键 0.50秒

请观察当前画面，判断控制效果
（例如：画面是否移动了？目标是否还在视野中？）
```

**优势**：
1. ✅ 简洁明了，减少 token 消耗
2. ✅ 只告诉控制动作，不告诉历史观察 → 防止幻觉
3. ✅ 强调"当前画面"，让模型专注于现在
4. ✅ 给出判断提示（画面是否移动、目标是否在视野中）

---

## 💡 关键改进点

### 1. **历史信息最小化**
```python
# 优化前：告诉了上次看到的内容
history_context = f"""
- **上次控制**: {last['control_key']} 键 {last['control_duration']:.2f}秒
- **上次看到**: {last['summary']}  ← 删除这行！
"""

# 优化后：只告诉控制动作
history_context = f"""
我刚才执行了：{last['control_key']} 键 {last['control_duration']:.2f}秒
"""
```

### 2. **强调观察当前画面**
```python
# 引导模型：
# - 观察当前画面
# - 判断控制效果（画面移动了吗？目标还在吗？）
# - 决定下一步动作
```

### 3. **提示词结构化**
使用清晰的层级结构：
- 任务目标（1 行）
- 环境说明（4 行）
- 控制按键（表格形式）
- 控制时长（2 行）
- 输出格式（示例 JSON）

---

## 📈 预期效果

### **场景 1：连续旋转搜索**
```
迭代 1:
  控制：K 0.5秒（右转）
  观察：看到蓝色地面
  
迭代 2:
  历史：刚才执行了 K 0.5秒
  观察：还是蓝色地面（说明需要继续转）
  控制：K 0.5秒（继续右转）
  
迭代 3:
  历史：刚才执行了 K 0.5秒
  观察：看到黑色圆盘（找到植物底座了！）
  控制：J 0.3秒（抬头看）
```

### **场景 2：目标丢失后恢复**
```
迭代 5:
  历史：刚才执行了 K 0.8秒
  观察：只看到紫色天空（转太多了，目标丢失）
  控制：I 0.5秒（反向转回去）
  
迭代 6:
  历史：刚才执行了 I 0.5秒
  观察：看到绿色植物边缘（找回来了）
  控制：I 0.2秒（继续微调对准）
```

**关键**：模型通过对比"上次控制动作"和"当前画面"，理解了控制效果，而不是依赖历史观察记录。

---

## 🧪 测试建议

1. **对比测试**：用相同场景分别测试优化前后的效果
2. **多次运行**：运行 5-10 次，统计成功率
3. **观察日志**：看模型的 summary 是否准确反映当前画面
4. **检查幻觉**：看是否出现"上次看到的内容"混入当前描述

---

## 📝 文件修改清单

- ✅ `qwen-vision-control.py` - 第 40-77 行：简化系统提示词
- ✅ `qwen-vision-control.py` - 第 165-178 行：优化历史上下文

---

## 🚀 下一步优化建议

1. **增加反馈机制**：如果 3 次迭代都没看到目标，提示"可能需要改变策略"
2. **动态调整控制时长**：根据"目标距离画面中心的距离"动态调整 duration
3. **记录失败模式**：统计哪些情况容易失败，针对性优化提示词
